"""
Syst√®me RAG principal pour le r√®glement technique.
Combine chunking, embeddings, ChromaDB et LLM pour r√©pondre aux questions.
"""

import os
from pathlib import Path
from typing import List, Dict, Tuple
from sentence_transformers import SentenceTransformer, util
import chromadb
from chromadb.config import Settings
from langchain_ollama import OllamaLLM
from langchain.prompts import PromptTemplate

from chunking import parse_regulation_to_chunks, load_chunks_from_txt, save_chunks_to_txt


class RAGSystem:
    """Syst√®me RAG pour le r√®glement technique."""
    
    def __init__(
        self, 
        embedding_model: str = 'multi-qa-mpnet-base-dot-v1',
        llm_model: str = 'llama3.2:latest',
        chroma_db_path: str = './data/chroma_db'
    ):
        """
        Initialise le syst√®me RAG.
        
        Args:
            embedding_model: Mod√®le SentenceTransformer pour les embeddings
            llm_model: Mod√®le Ollama pour la g√©n√©ration de r√©ponses
            chroma_db_path: Chemin de la base de donn√©es ChromaDB
        """
        print("üîÑ Initialisation du syst√®me RAG...")
        
        # Mod√®le d'embeddings
        print(f"üì¶ Chargement du mod√®le d'embeddings: {embedding_model}")
        self.embedding_model = SentenceTransformer(embedding_model)
        
        # ChromaDB
        print(f"üíæ Initialisation de ChromaDB: {chroma_db_path}")
        self.chroma_client = chromadb.PersistentClient(
            path=chroma_db_path,
            settings=Settings(anonymized_telemetry=False)
        )
        
        # Collection ChromaDB
        self.collection_name = "regulation_collection"
        try:
            self.collection = self.chroma_client.get_collection(name=self.collection_name)
            print(f"‚úÖ Collection existante charg√©e: {self.collection_name}")
        except:
            self.collection = self.chroma_client.create_collection(
                name=self.collection_name,
                metadata={"hnsw:space": "cosine"}
            )
            print(f"‚úÖ Nouvelle collection cr√©√©e: {self.collection_name}")
        
        # LLM
        print(f"ü§ñ Initialisation du LLM: {llm_model}")
        self.llm = OllamaLLM(model=llm_model, temperature=0.1)
        
        # Prompt template
        self.prompt_template = PromptTemplate(
            input_variables=["context", "question"],
            template="""Vous √™tes un expert en r√®glements techniques. Bas√© sur la documentation fournie, r√©pondez √† la question de mani√®re claire et pr√©cise en fran√ßais ou en russe selon la langue de la question.

Documentation:
{context}

Question: {question}

R√©ponse (soyez pr√©cis, citez les articles et points pertinents):"""
        )
        
        # Cha√Æne de traitement
        self.chain = self.prompt_template | self.llm
        
        print("‚úÖ Syst√®me RAG initialis√© avec succ√®s!\n")
    
    def load_regulation(self, regulation_file: str) -> List[Dict]:
        """
        Charge et parse le r√®glement.
        
        Args:
            regulation_file: Chemin vers le fichier du r√®glement
            
        Returns:
            Liste de chunks
        """
        print(f"üìñ Chargement du r√®glement: {regulation_file}")
        
        with open(regulation_file, 'r', encoding='utf-8') as f:
            text = f.read()
        
        print("üîç Parsing du r√®glement en chunks...")
        chunks = parse_regulation_to_chunks(text)
        print(f"‚úÖ {len(chunks)} chunks cr√©√©s")
        
        return chunks
    
    def index_chunks(self, chunks: List[Dict], force_reindex: bool = False) -> None:
        """
        Indexe les chunks dans ChromaDB avec leurs embeddings.
        
        Args:
            chunks: Liste de chunks √† indexer
            force_reindex: Si True, r√©indexe m√™me si la collection n'est pas vide
        """
        # V√©rifier si la collection est d√©j√† remplie
        if self.collection.count() > 0 and not force_reindex:
            print(f"‚ÑπÔ∏è  Collection d√©j√† index√©e avec {self.collection.count()} documents")
            return
        
        print(f"üîÑ Indexation de {len(chunks)} chunks...")
        
        # Extraire les textes
        documents = [chunk['text'] for chunk in chunks]
        
        # G√©n√©rer les embeddings
        print("üßÆ G√©n√©ration des embeddings...")
        embeddings = self.embedding_model.encode(documents, show_progress_bar=True)
        
        # Pr√©parer les m√©tadonn√©es
        metadatas = [
            {
                'id': chunk['id'],
                'article_num': chunk['article_num'],
                'article_title': chunk['article_title'],
                'point_num': chunk['point_num']
            }
            for chunk in chunks
        ]
        
        # IDs uniques
        ids = [f"chunk_{i}" for i in range(len(chunks))]
        
        # Ajouter √† ChromaDB
        print("üíæ Ajout √† ChromaDB...")
        self.collection.add(
            embeddings=embeddings.tolist(),
            documents=documents,
            metadatas=metadatas,
            ids=ids
        )
        
        print(f"‚úÖ {len(chunks)} chunks index√©s dans ChromaDB\n")
    
    def retrieve_context(self, question: str, n_results: int = 5) -> Tuple[str, List[str], List[Dict]]:
        """
        R√©cup√®re le contexte pertinent pour une question.
        
        Args:
            question: Question de l'utilisateur
            n_results: Nombre de r√©sultats √† retourner
            
        Returns:
            Tuple (context, documents, metadatas)
        """
        # G√©n√©rer l'embedding de la question
        query_embedding = self.embedding_model.encode([question])
        
        # Rechercher dans ChromaDB
        results = self.collection.query(
            query_embeddings=query_embedding.tolist(),
            n_results=n_results,
            include=["documents", "metadatas", "distances"]
        )
        
        # Extraire les r√©sultats
        documents = results["documents"][0]
        metadatas = results["metadatas"][0]
        
        # Cr√©er le contexte
        context = "\n\n---SECTION---\n\n".join(documents)
        
        return context, documents, metadatas
    
    def get_llm_answer(self, question: str, context: str) -> str:
        """
        G√©n√®re une r√©ponse en utilisant le LLM.
        
        Args:
            question: Question de l'utilisateur
            context: Contexte r√©cup√©r√©
            
        Returns:
            R√©ponse g√©n√©r√©e
        """
        answer = self.chain.invoke({
            "context": context[:4000],  # Limiter le contexte
            "question": question
        })
        return answer
    
    def stream_llm_answer(self, question: str, context: str):
        """
        G√©n√®re une r√©ponse en streaming.
        
        Args:
            question: Question de l'utilisateur
            context: Contexte r√©cup√©r√©
            
        Yields:
            Tokens de la r√©ponse
        """
        for token in self.llm.stream(
            self.prompt_template.format(context=context[:4000], question=question)
        ):
            yield token
    
    def format_response(
        self, 
        question: str, 
        answer: str, 
        source_chunks: List[str], 
        metadatas: List[Dict]
    ) -> str:
        """
        Formate la r√©ponse finale avec les sources.
        
        Args:
            question: Question de l'utilisateur
            answer: R√©ponse g√©n√©r√©e
            source_chunks: Chunks sources
            metadatas: M√©tadonn√©es des chunks
            
        Returns:
            R√©ponse format√©e
        """
        response = f"**Question:** {question}\n\n"
        response += f"**R√©ponse:** {answer}\n\n"
        response += "**Sources:**\n"
        
        for i, (chunk, metadata) in enumerate(zip(source_chunks[:3], metadatas[:3]), 1):
            article = metadata.get('article_num', 'N/A')
            point = metadata.get('point_num', 'N/A')
            title = metadata.get('article_title', 'N/A')
            preview = chunk[:150].replace("\n", " ") + "..."
            response += f"\n{i}. **Article {article}, Point {point}** ({title})\n"
            response += f"   {preview}\n"
        
        return response
    
    def query(self, question: str, n_results: int = 5) -> str:
        """
        Interroge le syst√®me RAG avec une question.
        
        Args:
            question: Question de l'utilisateur
            n_results: Nombre de chunks √† r√©cup√©rer
            
        Returns:
            R√©ponse format√©e
        """
        # R√©cup√©rer le contexte
        context, documents, metadatas = self.retrieve_context(question, n_results)
        
        # G√©n√©rer la r√©ponse
        answer = self.get_llm_answer(question, context)
        
        # Formater la r√©ponse
        return self.format_response(question, answer, documents, metadatas)
    
    def query_streaming(self, question: str, n_results: int = 5):
        """
        Interroge le syst√®me RAG avec streaming.
        
        Args:
            question: Question de l'utilisateur
            n_results: Nombre de chunks √† r√©cup√©rer
            
        Yields:
            Parties de la r√©ponse
        """
        # R√©cup√©rer le contexte
        context, documents, metadatas = self.retrieve_context(question, n_results)
        
        # G√©n√©rer la r√©ponse en streaming
        response_start = f"**Question:** {question}\n\n**R√©ponse:** "
        answer = ""
        
        for token in self.stream_llm_answer(question, context):
            answer += token
            yield response_start + answer
        
        # Ajouter les sources
        yield self.format_response(question, answer, documents, metadatas)


if __name__ == "__main__":
    # Test du syst√®me
    rag = RAGSystem()
    
    # Charger le r√®glement (exemple)
    # chunks = rag.load_regulation("data/regulation.txt")
    # rag.index_chunks(chunks)
    
    # Test d'une question
    # response = rag.query("–ß—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç—Å—è –æ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –≤ —Å—Ç–∞—Ç—å–µ 5?")
    # print(response)
