# ğŸ¤– Application RAG - RÃ¨glement Technique

Application de Question-RÃ©ponse intelligente basÃ©e sur le systÃ¨me RAG (Retrieval-Augmented Generation) pour interroger le rÃ¨glement technique.

## ğŸ“‹ CaractÃ©ristiques

- âœ… **Recherche sÃ©mantique** : Utilise SentenceTransformers pour comprendre le sens des questions
- âœ… **Base de donnÃ©es vectorielle** : ChromaDB pour un stockage et une recherche efficaces
- âœ… **LLM local** : Llama 3.2 via Ollama pour des rÃ©ponses intelligentes
- âœ… **Interface moderne** : Gradio pour une expÃ©rience utilisateur intuitive
- âœ… **DÃ©ploiement Docker** : Conteneurisation complÃ¨te avec docker-compose
- âœ… **Support multilingue** : FranÃ§ais et Russe

## ğŸ—ï¸ Architecture

```
rag-app/
â”œâ”€â”€ app.py                  # Application Gradio principale
â”œâ”€â”€ rag_system.py          # SystÃ¨me RAG (embeddings, ChromaDB, LLM)
â”œâ”€â”€ chunking.py            # Parsing et chunking du rÃ¨glement
â”œâ”€â”€ init_database.py       # Initialisation de la base de donnÃ©es
â”œâ”€â”€ requirements.txt       # DÃ©pendances Python
â”œâ”€â”€ Dockerfile            # Image Docker de l'application
â”œâ”€â”€ docker-compose.yml    # Configuration multi-conteneurs
â”œâ”€â”€ .gitignore           # Fichiers Ã  ignorer
â”œâ”€â”€ data/                # DonnÃ©es (rÃ¨glement, chunks, ChromaDB)
â””â”€â”€ models/              # ModÃ¨les tÃ©lÃ©chargÃ©s
```

## ğŸš€ Installation

### Option 1 : Avec Docker (RecommandÃ©)

#### PrÃ©requis
- Docker Desktop installÃ©
- Docker Compose installÃ©
- 8 GB RAM minimum

#### Ã‰tapes

1. **Cloner le dÃ©pÃ´t ou naviguer vers le dossier**
   ```bash
   cd rag-app
   ```

2. **TÃ©lÃ©charger le rÃ¨glement** (si pas dÃ©jÃ  fait)
   ```bash
   # Placez votre fichier regulation.txt dans le dossier data/
   # Ou exÃ©cutez le script d'initialisation (voir ci-dessous)
   ```

3. **Lancer l'application avec Docker Compose**
   ```bash
   docker-compose up --build
   ```

   Cette commande va :
   - Construire l'image Docker de l'application
   - DÃ©marrer le conteneur Ollama
   - TÃ©lÃ©charger le modÃ¨le Llama 3.2
   - DÃ©marrer l'application Gradio

4. **AccÃ©der Ã  l'application**
   - Ouvrez votre navigateur : http://localhost:7860
   - L'application est prÃªte Ã  l'emploi !

### Option 2 : Installation Locale

#### PrÃ©requis
- Python 3.10+
- Ollama installÃ© ([https://ollama.ai](https://ollama.ai))

#### Ã‰tapes

1. **Installer les dÃ©pendances**
   ```bash
   pip install -r requirements.txt
   ```

2. **TÃ©lÃ©charger le modÃ¨le Llama 3.2**
   ```bash
   ollama pull llama3.2:latest
   ```

3. **Initialiser la base de donnÃ©es**
   ```bash
   python init_database.py
   ```

   Ce script va :
   - TÃ©lÃ©charger le rÃ¨glement depuis Google Drive
   - Le parser en chunks structurÃ©s
   - CrÃ©er les embeddings
   - Indexer dans ChromaDB

4. **Lancer l'application**
   ```bash
   python app.py
   ```

5. **AccÃ©der Ã  l'application**
   - Ouvrez votre navigateur : http://localhost:7860

## ğŸ“– Utilisation

### Interface Web

1. **Posez une question** dans la zone de texte
2. **Attendez la rÃ©ponse** (streaming en temps rÃ©el)
3. **Consultez les sources** citÃ©es avec les articles et points rÃ©fÃ©rencÃ©s

### Exemples de questions

```
ğŸ‡·ğŸ‡º Russe:
- "Ğ§Ñ‚Ğ¾ Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¸Ñ‚ÑÑ Ğ² ÑÑ‚Ğ°Ñ‚ÑŒĞµ 5 Ğ¾ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑÑ… Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸?"
- "ĞšĞ°ĞºĞ¸Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ?"
- "ĞšĞ°Ğº Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ»ÑĞµÑ‚ÑÑ Ğ¼Ğ°Ñ€ĞºĞ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ†Ğ¸Ğ¸?"

ğŸ‡«ğŸ‡· FranÃ§ais:
- "Quelles sont les exigences de sÃ©curitÃ© pour les produits?"
- "Comment s'effectue la certification?"
- "Quels sont les documents requis?"
```

### API Python

```python
from rag_system import RAGSystem

# Initialiser le systÃ¨me
rag = RAGSystem()

# Charger et indexer le rÃ¨glement
chunks = rag.load_regulation("data/regulation.txt")
rag.index_chunks(chunks)

# Poser une question
response = rag.query("Ğ§Ñ‚Ğ¾ Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¸Ñ‚ÑÑ Ğ¾ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸?")
print(response)

# Avec streaming
for token in rag.query_streaming("Quelles sont les exigences?"):
    print(token, end="", flush=True)
```

## ğŸ”§ Configuration

### Variables d'environnement

```bash
# HÃ´te Ollama (pour Docker)
OLLAMA_HOST=http://ollama:11434

# Configuration Gradio
GRADIO_SERVER_NAME=0.0.0.0
GRADIO_SERVER_PORT=7860
```

### Personnalisation du systÃ¨me RAG

Dans [rag_system.py](rag_system.py), vous pouvez modifier :

```python
RAGSystem(
    embedding_model='multi-qa-mpnet-base-dot-v1',  # ModÃ¨le d'embeddings
    llm_model='llama3.2:latest',                    # ModÃ¨le LLM
    chroma_db_path='./data/chroma_db'              # Chemin ChromaDB
)
```

## ğŸ“Š MÃ©thode de Chunking

Le systÃ¨me utilise une mÃ©thode de chunking structurÃ©e basÃ©e sur la structure du rÃ¨glement :

1. **Identification des articles** : Extraction des numÃ©ros et titres d'articles
2. **DÃ©coupage par points** : Chaque point devient un chunk
3. **MÃ©tadonnÃ©es riches** : 
   - ID du chunk (ex: "5.3" pour Article 5, Point 3)
   - NumÃ©ro d'article
   - Titre d'article
   - NumÃ©ro de point
   - Texte complet

Cette mÃ©thode prÃ©serve la structure hiÃ©rarchique du rÃ¨glement et facilite les citations prÃ©cises.

## ğŸ” Workflow RAG

```mermaid
graph LR
    A[Question] --> B[Embedding]
    B --> C[Recherche Vectorielle]
    C --> D[Top-K Chunks]
    D --> E[LLM]
    E --> F[RÃ©ponse + Sources]
```

1. **Question** â†’ Transformation en embedding vectoriel
2. **Recherche** â†’ Comparaison avec les embeddings des chunks
3. **RÃ©cupÃ©ration** â†’ Top-K chunks les plus pertinents
4. **GÃ©nÃ©ration** â†’ LLM gÃ©nÃ¨re une rÃ©ponse basÃ©e sur le contexte
5. **Formatage** â†’ RÃ©ponse + citations des sources

## ğŸ³ Commandes Docker Utiles

```bash
# DÃ©marrer l'application
docker-compose up

# DÃ©marrer en arriÃ¨re-plan
docker-compose up -d

# Voir les logs
docker-compose logs -f

# ArrÃªter l'application
docker-compose down

# Reconstruire les images
docker-compose build --no-cache

# Supprimer volumes et tout reconstruire
docker-compose down -v
docker-compose up --build
```

## ğŸ› ï¸ RÃ©solution de ProblÃ¨mes

### L'application ne dÃ©marre pas

1. VÃ©rifier que Docker Desktop est en cours d'exÃ©cution
2. VÃ©rifier les ports 7860 et 11434 sont disponibles
3. VÃ©rifier les logs : `docker-compose logs`

### Ollama ne rÃ©pond pas

```bash
# VÃ©rifier le statut
docker-compose ps

# RedÃ©marrer Ollama
docker-compose restart ollama
```

### ChromaDB vide

```bash
# RÃ©initialiser la base de donnÃ©es
python init_database.py
```

### ProblÃ¨mes de mÃ©moire

- Minimum 8 GB RAM requis
- Augmenter la mÃ©moire allouÃ©e Ã  Docker Desktop

## ğŸ“¦ Structure des DonnÃ©es

```
data/
â”œâ”€â”€ regulation.txt          # RÃ¨glement technique original
â”œâ”€â”€ chunks.txt             # Chunks formatÃ©s avec mÃ©tadonnÃ©es
â””â”€â”€ chroma_db/            # Base de donnÃ©es vectorielle ChromaDB
    â”œâ”€â”€ chroma.sqlite3
    â””â”€â”€ ...
```

## ğŸ¤ Contribution

Les contributions sont les bienvenues ! N'hÃ©sitez pas Ã  :

1. Fork le projet
2. CrÃ©er une branche (`git checkout -b feature/amelioration`)
3. Commit vos changements (`git commit -m 'Ajout de fonctionnalitÃ©'`)
4. Push vers la branche (`git push origin feature/amelioration`)
5. Ouvrir une Pull Request

## ğŸ“ Licence

Ce projet est distribuÃ© sous licence MIT.

## ğŸ‘¥ Auteurs

- DÃ©veloppÃ© en utilisant les mÃ©thodes des notebooks :
  - `Ğ›Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ_Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸_Ğ´Ğ»Ñ_Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ_ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ²_Ğ¸_Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğµ_Ğ‘Ğ” (2).ipynb`
  - `rag-llama3-2-gradio.ipynb`

## ğŸ™ Remerciements

- **SentenceTransformers** pour les embeddings
- **ChromaDB** pour la base vectorielle
- **Ollama** pour l'hÃ©bergement local des LLM
- **Gradio** pour l'interface utilisateur
- **LangChain** pour l'orchestration RAG

## ğŸ“ Support

Pour toute question ou problÃ¨me, veuillez ouvrir une issue sur GitHub.

---

**Fait avec â¤ï¸ et ğŸ¤– IA**
